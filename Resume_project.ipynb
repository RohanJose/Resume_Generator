{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfminer.six\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76gZujp_2_yJ",
        "outputId": "b0c24624-e0be-44d1-8745-68ada4c3e7f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfminer.six\n",
            "  Downloading pdfminer.six-20240706-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (3.3.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (43.0.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six) (1.17.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.22)\n",
            "Downloading pdfminer.six-20240706-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pdfminer.six\n",
            "Successfully installed pdfminer.six-20240706\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pdfminer.high_level import extract_text\n",
        "import google.generativeai as genai\n",
        "\n",
        "#Either add your api key as secrets or just r\n",
        "GOOGLE_API_KEY=\"PAsTE YOUR API KEY\"\n",
        "\n",
        "\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "model = genai.GenerativeModel('gemini-1.5-flash')\n",
        "\n",
        "def get_gemini_response(prompt):\n",
        "   return model.generate_content(prompt)\n",
        "\n",
        "def parse_resume(resume_path):\n",
        "    \"\"\"Parse resume PDF to extract text\"\"\"\n",
        "    return extract_text(resume_path)\n",
        "\n",
        "\n",
        "resume_text = parse_resume(\"/content/Vinod Jangid Resume.pdf\")\n",
        "job_description=\"\"\"We are looking for a Front-End Web Developer who is motivated to combine the art of design with the art of programming. Responsibilities will include translation of the UI/UX design wireframes to actual code that will produce visual elements of the application. You will work with the UI/UX designer and bridge the gap between graphical design and technical implementation, taking an active role on both sides and defining how the application looks as well as how it works.\n",
        "\"\"\"\n",
        "\n",
        "input_prompt = \"\"\"\n",
        "    You are an skilled ATS (Applicant Tracking System) scanner with a deep understanding in the field of computer sciecnce,\n",
        "    your task is to evaluate the resume against the provided job description. You should output first the percentage of match between the resume and\n",
        "    the job description, followed by keywords matching, keywords missing, specific changes for resume to reach matching percentage\n",
        "    of above 85 and lastly your final thoughts.\n",
        "    Resume: {}\n",
        "    Job Description: {}\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "prompt=input_prompt.format(resume_text, job_description)\n",
        "response=get_gemini_response(prompt)\n",
        "for chunk in response:\n",
        "  print(chunk.text)\n",
        "  print(\"_\"*80)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 662
        },
        "id": "n8hooLxNuUeZ",
        "outputId": "a7e72475-e127-4f16-ebbb-4c6d98f1e379"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## Resume Evaluation\n",
            "\n",
            "**Match Percentage:** 45%\n",
            "\n",
            "**Keywords Matching:**\n",
            "\n",
            "* **Frontend Development:**  Present in both resume and job description.\n",
            "* **HTML:** Present in resume skills section.\n",
            "* **CSS:** Present in resume skills section.\n",
            "* **JavaScript:** Present in resume skills section.\n",
            "* **React JS:** Present in resume skills section.\n",
            "* **UI/UX:** Mentioned in resume projects (personal portfolio, college fest website, Netflix clone) but not explicitly as a skill. \n",
            "* **Design:** Present in resume projects (personal portfolio, college fest website, Netflix clone) but not explicitly as a skill.\n",
            "\n",
            "**Keywords Missing:**\n",
            "\n",
            "* **UI/UX Design:** Not explicitly mentioned as a skill on the resume.\n",
            "* **Wireframes:** Not mentioned. \n",
            "* **Technical Implementation:** Not mentioned.\n",
            "* **Application:**  Not explicitly mentioned.\n",
            "* **Visual Elements:** Not mentioned. \n",
            "* **Bridge the Gap:**  Not mentioned. \n",
            "\n",
            "**Specific Changes for Resume:**\n",
            "\n",
            "* **Highlight UI/UX Design Skills:**  Emphasize experience with UI/UX design principles and tools, including Figma and Canva, in the skills section and project descriptions.  \n",
            "* **Showcase Technical Implementation:** Include details about how you translate designs into code, using specific tools and technologies. For example: \"Implemented UI wireframes using React JS and CSS for a responsive user interface.\" \n",
            "* **Add Specific Design Skills:**  Mention proficiency in Adobe Photoshop and Illustrator (or any other relevant design tools) in the skills section.\n",
            "* **Quantify Achievements:**  Add details about the size and complexity of the projects you've worked on to demonstrate your experience.  For example: \"Developed a web application using React JS with over 100 components, resulting in a 20% improvement in loading time.\"\n",
            "* **Focus on Collaboration:**  Emphasize your ability to collaborate with designers, as this is a key aspect of the job description. For example: \"Worked closely with UI/UX designers to translate design wireframes into functional code.\"\n",
            "\n",
            "**Final Thoughts:**\n",
            "\n",
            "The resume demonstrates strong technical skills in frontend development. However, it needs to be tailored to highlight experience and skills related to UI/UX design and collaboration with designers. By emphasizing these areas, the candidate can increase their chances of passing the ATS and landing an interview. \n",
            "\n",
            "________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-6wRD1_3v1F",
        "outputId": "c257cb4d-5969-45e7-b0e2-845114a9357d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-4.43.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Collecting fastapi<0.113.0 (from gradio)\n",
            "  Downloading fastapi-0.112.4-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.4.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting gradio-client==1.3.0 (from gradio)\n",
            "  Downloading gradio_client-1.3.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting httpx>=0.24.1 (from gradio)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.24.6)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.4.4)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.4)\n",
            "Collecting orjson~=3.0 (from gradio)\n",
            "  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.1)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.4)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (9.4.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.8.2)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.9 (from gradio)\n",
            "  Downloading python_multipart-0.0.9-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.2.2 (from gradio)\n",
            "  Downloading ruff-0.6.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting tomlkit==0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.5)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.0.7)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.30.6-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.3.0->gradio) (2024.6.1)\n",
            "Collecting websockets<13.0,>=10.0 (from gradio-client==1.3.0->gradio)\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.8)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
            "Collecting starlette<0.39.0,>=0.37.2 (from fastapi<0.113.0->gradio)\n",
            "  Downloading starlette-0.38.4-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.8.30)\n",
            "Collecting httpcore==1.* (from httpx>=0.24.1->gradio)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.24.1->gradio)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (3.15.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (4.66.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.20.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.8.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.16.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.3.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-4.43.0-py3-none-any.whl (18.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.3.0-py3-none-any.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.7/318.7 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.112.4-py3-none-any.whl (93 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.9/93.9 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
            "Downloading ruff-0.6.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.3/10.3 MB\u001b[0m \u001b[31m70.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading uvicorn-0.30.6-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading starlette-0.38.4-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.4/71.4 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydub, websockets, tomlkit, semantic-version, ruff, python-multipart, orjson, h11, ffmpy, aiofiles, uvicorn, starlette, httpcore, httpx, fastapi, gradio-client, gradio\n",
            "  Attempting uninstall: tomlkit\n",
            "    Found existing installation: tomlkit 0.13.2\n",
            "    Uninstalling tomlkit-0.13.2:\n",
            "      Successfully uninstalled tomlkit-0.13.2\n",
            "Successfully installed aiofiles-23.2.1 fastapi-0.112.4 ffmpy-0.4.0 gradio-4.43.0 gradio-client-1.3.0 h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 orjson-3.10.7 pydub-0.25.1 python-multipart-0.0.9 ruff-0.6.4 semantic-version-2.10.0 starlette-0.38.4 tomlkit-0.12.0 uvicorn-0.30.6 websockets-12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import google.generativeai as genai\n",
        "from pdfminer.high_level import extract_text\n",
        "\n",
        "# Configure Google Gemini API\n",
        "GOOGLE_API_KEY = \"AIzaSyAEe556kHS877qjaGj6XIqc_T9qp0ZhJnQ\"\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "model = genai.GenerativeModel('gemini-1.5-flash')\n",
        "\n",
        "def get_gemini_response(prompt):\n",
        "    \"\"\"Generate content using Google Gemini.\"\"\"\n",
        "    return model.generate_content(prompt)\n",
        "\n",
        "def parse_resume(resume_file):\n",
        "    \"\"\"Parse resume PDF to extract text.\"\"\"\n",
        "    resume_text = extract_text(resume_file.name)\n",
        "    return resume_text\n",
        "\n",
        "def evaluate_resume(resume_file, job_description):\n",
        "    \"\"\"Evaluate resume against job description.\"\"\"\n",
        "    resume_text = parse_resume(resume_file)\n",
        "    input_prompt = f\"\"\"\n",
        "    You are a skilled ATS (Applicant Tracking System) scanner with a deep understanding in the field of computer science.\n",
        "    Your task is to evaluate the resume against the provided job description. You should output first the percentage of match between the resume and\n",
        "    the job description, followed by keywords matching, keywords missing, specific changes for resume to reach a matching percentage\n",
        "    of above 85, and lastly your final thoughts.\n",
        "    Resume: {resume_text}\n",
        "    Job Description: {job_description}\n",
        "    \"\"\"\n",
        "\n",
        "    response = get_gemini_response(input_prompt)\n",
        "    resp = \"\"\n",
        "    for chunk in response:\n",
        "      resp += chunk.text\n",
        "    return resp\n",
        "\n",
        "# Define Gradio interface\n",
        "def run_evaluation(resume_file, job_description):\n",
        "    \"\"\"Run the evaluation and return the result.\"\"\"\n",
        "\n",
        "    resume_text = parse_resume(resume_file)\n",
        "    return evaluate_resume(resume_text, job_description)\n",
        "\n",
        "# Create Gradio interface\n",
        "interface = gr.Interface(\n",
        "    fn=run_evaluation,\n",
        "    inputs=[\n",
        "        gr.File(label=\"Upload Resume (PDF)\"),\n",
        "        gr.Textbox(label=\"Job Description\", lines=5)\n",
        "    ],\n",
        "    outputs=\"text\",\n",
        "    title=\"Resume Evaluation\",\n",
        "    description=\"Upload a resume and enter a job description. Click the button below to evaluate the resume against the job description.\",\n",
        "    live=False\n",
        ")\n",
        "\n",
        "# Launch the interface\n",
        "interface.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "y73vj5_O3ugw",
        "outputId": "6117ca4a-dbbd-4e35-9794-395373b24686"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://81fec6d152632bb7e7.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://81fec6d152632bb7e7.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import textacy\n",
        "from pdfminer.high_level import extract_text\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import numpy as np\n",
        "\n",
        "# Load SentenceTransformer model\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')  # You can use other pre-trained models as needed\n",
        "\n",
        "\n",
        "def parse_resume(resume_path):\n",
        "    \"\"\"Parse resume PDF to extract text\"\"\"\n",
        "    return extract_text(resume_path)\n",
        "\n",
        "def extract_keywords(job_description):\n",
        "    \"\"\"\n",
        "    Extract keywords from a job description using spaCy and textacy.\n",
        "\n",
        "    Args:\n",
        "        job_description (str): The job description text.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of extracted keywords.\n",
        "    \"\"\"\n",
        "    # Create a spaCy document\n",
        "    doc = nlp(job_description)\n",
        "\n",
        "    # Extract keywords using textacy's keyterms extraction\n",
        "    key_terms = textacy.extract.keyterms.sgrank(doc, normalize='lemma')\n",
        "\n",
        "    # Collect and return keywords from the key_terms extraction\n",
        "    keywords = [term for term, score in key_terms]\n",
        "\n",
        "    return keywords\n",
        "\n",
        "def extract_key_terms(job_description):\n",
        "    \"\"\"Extract key terms from job description\"\"\"\n",
        "    doc = textacy.make_spacy_doc(job_description)\n",
        "    return textacy.extract.keyterms.sgrank(doc)\n",
        "\n",
        "def calculate_similarity(resume_text, job_description):\n",
        "    \"\"\"Calculate vector similarity between resume and job description using SentenceTransformers\"\"\"\n",
        "    # Generate embeddings for the resume and job description\n",
        "    resume_embedding = model.encode(resume_text, convert_to_tensor=True)\n",
        "    job_description_embedding = model.encode(job_description, convert_to_tensor=True)\n",
        "\n",
        "    # Compute cosine similarity\n",
        "    similarity = util.pytorch_cos_sim(resume_embedding, job_description_embedding).item()\n",
        "    return similarity\n",
        "\n",
        "def process_resume_and_job_description(resume_path, job_description):\n",
        "    \"\"\"Main function to process resume and job description\"\"\"\n",
        "    resume_text = parse_resume(resume_path)\n",
        "    keywords = extract_keywords(job_description)\n",
        "    key_terms = extract_key_terms(job_description)\n",
        "    similarity = calculate_similarity(resume_text, job_description)\n",
        "\n",
        "    return {\n",
        "        \"resume_text\": resume_text,\n",
        "        \"keywords\": keywords,\n",
        "        \"key_terms\": key_terms,\n",
        "        \"similarity_score\": similarity\n",
        "    }\n",
        "\n",
        "# Define Gradio interface\n",
        "def gradio_interface(resume_file, job_description):\n",
        "    result = process_resume_and_job_description(resume_file.name, job_description)\n",
        "\n",
        "    return (result[\"resume_text\"], result[\"keywords\"], result[\"key_terms\"], result[\"similarity_score\"])\n",
        "\n",
        "interface = gr.Interface(\n",
        "    fn=gradio_interface,\n",
        "    inputs=[gr.File(label=\"Upload Resume (PDF)\"), gr.Textbox(label=\"Job Description\")],\n",
        "    outputs=[gr.Textbox(label=\"Resume Text\"), gr.JSON(label=\"Keywords\"), gr.JSON(label=\"Key Terms\"), gr.Number(label=\"Similarity Score\")]\n",
        ")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    interface.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "JAz9sfAShChk",
        "outputId": "8e915021-d26e-4122-8314-cb1782c22c1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://897ec08fc371202d4d.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://897ec08fc371202d4d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1QQwzUFPiGjh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}